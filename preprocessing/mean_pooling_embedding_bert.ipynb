{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c41f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean pooling \n",
    "\n",
    "'''\n",
    ">>> Script for getting embeddings with mean pooling for better generalization\n",
    "IMPORTANT: The script currently loads the data from google drive. \n",
    "You can change input path, based on where your data is.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertModel.from_pretrained(model_name).half().to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  # (1, seq_len, hidden_size)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return (sum_embeddings / sum_mask).squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "def embed_comment(comment_text):\n",
    "    inputs = tokenizer(comment_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "def embed_long_comment(text, chunk_size=510, stride=128):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i:i+chunk_size]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        chunk = tokenizer.build_inputs_with_special_tokens(chunk.tolist())\n",
    "        chunk_inputs = torch.tensor([chunk]).to(\"cuda\")\n",
    "        attention_mask = (chunk_inputs != tokenizer.pad_token_id).long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(chunk_inputs)\n",
    "        chunk_embedding = mean_pooling(outputs, attention_mask)\n",
    "        chunks.append(chunk_embedding)\n",
    "\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "\n",
    "    return np.mean(chunks, axis=0)\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "    return embed_comment(text) if len(tokens) <= 512 else embed_long_comment(text)\n",
    "\n",
    "# currently loads the data from google drive. \n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "json_path = \"/content/drive/MyDrive/filtered_pandora.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "sample_size = int(len(data[\"authors\"]) * 0.10)\n",
    "sampled_authors = data[\"authors\"][:sample_size]\n",
    "for author in tqdm(sampled_authors):\n",
    "    author_id = author[\"id\"]\n",
    "    labels = author[\"labels\"]\n",
    "    comments = author.get(\"comments\", [])\n",
    "\n",
    "    embeddings = []\n",
    "    for comment in comments:\n",
    "        try:\n",
    "            vec = get_embedding(comment)\n",
    "            embeddings.append(vec)\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding comment for {author_id}: {e}\")\n",
    "\n",
    "    if embeddings:\n",
    "        avg_embedding = np.mean(embeddings, axis=0)\n",
    "        row = {\n",
    "            \"id\": author_id,\n",
    "            **labels\n",
    "        }\n",
    "        for i in range(len(avg_embedding)):\n",
    "            row[f\"embed_{i}\"] = avg_embedding[i]\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"author_embeddings_meanpool.csv\", index=False)\n",
    "print(\"Saved mean pooled embeddings to author_embeddings_meanpool.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
