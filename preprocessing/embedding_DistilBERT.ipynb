{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f34d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "**************************************************************************************************************\n",
    "**************************************************************************************************************\n",
    ">>>>>>>>>> Script for making embeddings with >>distilbert/distilbert-base-cased-distilled-squad<< \n",
    "The script will take all the comments, longer than 512 token comments will be chunked; \n",
    "each chunk will be embedded,then the vec is made of average.\n",
    "Run on Colab. If not in colab change loading method.\n",
    "Since on Colab, might need installing transformers and torch\n",
    "In case it takes too long you can embed 10% or more of the file. \n",
    "Mentioned in the script: change variable to embed bigger percentage of the file.\n",
    "Saves output in drive. <<<<<<<<<<<<<<<<\n",
    "**************************************************************************************************************\n",
    "**************************************************************************************************************\n",
    "'''\n",
    "\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "model = DistilBertModel.from_pretrained(model_name).half().to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# embedding function (for shorter texts)\n",
    "def embed_comment(comment_text):\n",
    "    inputs = tokenizer(comment_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "# embedding function (for longer texts)\n",
    "def embed_long_comment(text, chunk_size=510, stride=128):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i:i+chunk_size]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        \n",
    "        chunk = tokenizer.build_inputs_with_special_tokens(chunk.tolist())\n",
    "        chunk_inputs = torch.tensor([chunk]).to(\"cuda\")  # send to GPU\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(chunk_inputs)\n",
    "        \n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()  # move back to CPU\n",
    "        chunks.append(cls_embedding)\n",
    "        \n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    return np.mean(chunks, axis=0)\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "    return embed_comment(text) if len(tokens) <= 512 else embed_long_comment(text)\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "json_path = \"/content/drive/MyDrive/filtered_pandora.json\" # file name already in there; for val-set need new path\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "''' \n",
    "**************************************************************************************************************\n",
    "Use this if it takes too long (just to see if the model works)\n",
    "Change the variable 0.10 if you want to embed bigger percentage of the file\n",
    "\n",
    "sample_size = int(len(data[\"authors\"]) * 0.10)\n",
    "sampled_authors = data[\"authors\"][:sample_size]\n",
    "\n",
    "for author in tqdm(sampled_authors):\n",
    "\n",
    "**************************************************************************************************************\n",
    "\n",
    "'''\n",
    "\n",
    "for author in tqdm(data[\"authors\"]):\n",
    "    author_id = author[\"id\"]\n",
    "    labels = author[\"labels\"]\n",
    "    comments = author.get(\"comments\", [])\n",
    "\n",
    "\n",
    "    # averaging the embeddings of chunks for longer passeges\n",
    "    embeddings = []\n",
    "    \n",
    "    for comment in comments:\n",
    "        try:\n",
    "            vec = get_embedding(comment)\n",
    "            embeddings.append(vec)\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding comment for {author_id}: {e}\")\n",
    "\n",
    "    if embeddings:\n",
    "        avg_embedding = np.mean(embeddings, axis=0)\n",
    "        row = {\n",
    "            \"id\": author_id,\n",
    "            **labels\n",
    "        }\n",
    "        for i in range(len(avg_embedding)):\n",
    "            row[f\"embed_{i}\"] = avg_embedding[i]\n",
    "        rows.append(row)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"author_embeddings.csv\", index=False)\n",
    "print(\"Saved to author_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08354d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "**************************************************************************************************************\n",
    "**************************************************************************************************************\n",
    ">>>>>>>>>> Script for making embeddings from the validation with >>distilbert/distilbert-base-cased-distilled-squad<< \n",
    "The script will take all the comments, longer than 512 token comments will be chunked; \n",
    "each chunk will be embedded,then the vec is made of average.\n",
    "Run on Colab. If not in colab change loading method.\n",
    "Since on Colab, might need installing transformers and torch\n",
    "In case it takes too long you can embed 10% or more of the file. \n",
    "Mentioned in the script: change variable to embed bigger percentage of the file.\n",
    "Saves output in drive. <<<<<<<<<<<<<<<<\n",
    "**************************************************************************************************************\n",
    "**************************************************************************************************************\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertModel.from_pretrained(model_name).half().to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# Embedding functions\n",
    "def embed_comment(comment_text):\n",
    "    inputs = tokenizer(comment_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "def embed_long_comment(text, chunk_size=510, stride=128):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i:i+chunk_size]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        chunk = tokenizer.build_inputs_with_special_tokens(chunk.tolist())\n",
    "        chunk_inputs = torch.tensor([chunk]).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(chunk_inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "        chunks.append(cls_embedding)\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    return np.mean(chunks, axis=0)\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "    return embed_comment(text) if len(tokens) <= 512 else embed_long_comment(text)\n",
    "\n",
    "# Load val data\n",
    "val_df = pd.read_csv(\"/content/drive/MyDrive/val_data.csv\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for idx, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "    author_id = row.get(\"id\", f\"val_{idx}\")\n",
    "    full_text = \" \".join(str(row[q]) for q in ['Q1', 'Q2', 'Q3'] if pd.notna(row[q]))\n",
    "\n",
    "    try:\n",
    "        vec = get_embedding(full_text)\n",
    "        row_data = {\n",
    "            \"id\": author_id,\n",
    "            \"Openness\": row[\"Openness\"],\n",
    "            \"Conscientiousness\": row[\"Conscientiousness\"],\n",
    "            \"Extraversion\": row[\"Extraversion\"],\n",
    "            \"Agreeableness\": row[\"Agreeableness\"],\n",
    "            \"Emotional stability\": row[\"Emotional stability\"]\n",
    "        }\n",
    "        for i in range(len(vec)):\n",
    "            row_data[f\"embed_{i}\"] = vec[i]\n",
    "        rows.append(row_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding val sample {author_id}: {e}\")\n",
    "\n",
    "val_embed_df = pd.DataFrame(rows)\n",
    "val_embed_df.to_csv(\"/content/drive/MyDrive/val_embeddings.csv\", index=False)\n",
    "print(\"Saved to /content/drive/MyDrive/val_embeddings.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amiv_nlp_2025_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
